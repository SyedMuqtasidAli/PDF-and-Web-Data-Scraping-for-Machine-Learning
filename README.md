# PDF and Web Data Scraping for Machine Learning

Welcome to the EZline Machine Learning Repository! This repository features PDF data extraction using PyMuPDF and web scraping with BeautifulSoup, integrated with pandas, numpy, and MySQL for streamlined data processing.

## Table of Contents
- [Instructions for Running the Project](#instructions-for-running-the-project)
- [Installation](#installation)
- [Usage](#usage)
- [Contact](#contact)
- [License](#license)

## Instructions for Running the Project

### Download and Extract EZline.zip:
1. Download the `ezline.zip` file.
2. Extract the file to the directory where Anaconda is typically set up, often at `C:/Users/YourPCName/`.

### Start XAMPP Server:
1. Open XAMPP and start both Apache and MySQL servers.

### Open Anaconda Navigator:
1. Open Anaconda Navigator and launch Jupyter Lab.

### Run EZline Test Notebook:
1. Open the `EZline_test.ipynb` file in Jupyter Lab.
2. Run the notebook cells from start to end using SHIFT + ENTER.

### Run Flask Web Application:
1. Open Anaconda Prompt.
2. Navigate to the directory where `app.py` is located.
3. Run the script using the command: `python app.py`.

### Access Web Application:
1. Open the provided URL in your browser.
2. Click on the "Download Data" button to fetch the data.

### Watch the Instructional Video:
For detailed steps, refer to the instructional video provided in `Instruction Video.mp4`.

### Installation
To install the required dependencies, run the following command in Anaconda Prompt:
```sh
pip install -r requirements.txt
```
## Usage
Start the project following the steps mentioned above and use the provided tools and scripts for your data extraction and processing needs.

## Contact
Feel free to contact me on LinkedIn for any questions or collaborations:
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat-square&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/syed-muqtasid-ali-91a0a623a/)

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

